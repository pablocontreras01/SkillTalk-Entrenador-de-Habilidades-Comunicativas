{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTPaBQgwZFM18a73tTNj7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablocontreras01/SkillTalk-Entrenador-de-Habilidades-Comunicativas/blob/main/Modelo_Final_SkillTalk_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/Videos SkillTalk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1WQtcLteLuv",
        "outputId": "be1c3247-87e1-4c24-ca49-25a6ab0f10a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Videos SkillTalk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2y2g8QybrQl",
        "outputId": "2906a511-9b4c-442e-a485-7539f324158a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Par√°metros"
      ],
      "metadata": {
        "id": "DA_5-aQ4cYDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3voacPfBbFyd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "from collections import Counter\n",
        "\n",
        "# ====================================================================\n",
        "## ‚öôÔ∏è PAR√ÅMETROS DE CONFIGURACI√ìN\n",
        "# ====================================================================\n",
        "\n",
        "# üõë 1. RUTAS Y ARCHIVOS (AJUSTAR ESTO) üõë\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/Videos SkillTalk/Videos_Validaci√≥n/president-trump-s-midterm-rally-tour_qwl2YURk.mp4\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Videos SkillTalk/mlp_lstm_ted.h5\"\n",
        "OUTPUT_VIDEO_PATH = \"/content/drive/MyDrive/Videos SkillTalk/president-trump-s-midterm-rally-tour_qwl2YURk_retro.mp4\"\n",
        "\n",
        "# üõë 2. PAR√ÅMETROS DEL MODELO Y PROCESAMIENTO üõë\n",
        "CHUNK_SIZE = 30 # Tama√±o de la secuencia que espera tu modelo (L_MAX).\n",
        "CLASS_NAMES = [\"Beat\", \"No-Gesture\"] # Clases en el orden de salida del modelo (√çndice 0, 1)\n",
        "# Colores en formato BGR (Blue, Green, Red) para OpenCV\n",
        "COLORS = {\n",
        "    \"Beat\": (0, 255, 0),    # Verde (Gesto activo)\n",
        "    \"No-Gesture\": (255, 0, 0) # Azul (No-Gesture)\n",
        "}\n",
        "\n",
        "# üõë 3. CONSTANTES DEL ESQUELETO (Kinect v2) üõë\n",
        "# Necesarias para la normalizaci√≥n (Las mismas que usaste)\n",
        "SPINE_BASE = 0; SPINE_MID = 1; NECK = 2; HEAD = 3\n",
        "SHOULDER_LEFT = 4; ELBOW_LEFT = 5; WRIST_LEFT = 6; HAND_LEFT = 7\n",
        "SHOULDER_RIGHT = 8; ELBOW_RIGHT = 9; WRIST_RIGHT = 10; HAND_RIGHT = 11\n",
        "HIP_LEFT = 12; KNEE_LEFT = 13; ANKLE_LEFT = 14; FOOT_LEFT = 15\n",
        "HIP_RIGHT = 16; KNEE_RIGHT = 17; ANKLE_RIGHT = 18; FOOT_RIGHT = 19\n",
        "SPINE_SHOULDER = 20; HANDTIP_LEFT = 21; THUMB_LEFT = 22\n",
        "HANDTIP_RIGHT = 23; THUMB_RIGHT = 24\n",
        "\n",
        "# Inicializar MediaPipe\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocesamiento"
      ],
      "metadata": {
        "id": "ee2RgnY-cV3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## üìè FUNCI√ìN DE NORMALIZACI√ìN GEOM√âTRICA (ID√âNTICA AL ENTRENAMIENTO)\n",
        "def normalize_skeleton_sequence(seq: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Normaliza una secuencia completa de esqueletos (T, 25, 3).\"\"\"\n",
        "    seq = seq.copy().astype(np.float32)\n",
        "    seq[np.isnan(seq)] = 0.0\n",
        "\n",
        "    # 1. Centrar en pelvis (SPINE_BASE)\n",
        "    root = seq[:, SPINE_BASE:SPINE_BASE+1, :]\n",
        "    seq = seq - root\n",
        "\n",
        "    # 2. Rotaci√≥n para alinear hombros con eje X\n",
        "    left_shoulder = seq[:, SHOULDER_LEFT, :]\n",
        "    right_shoulder = seq[:, SHOULDER_RIGHT, :]\n",
        "    shoulder_vec = np.mean(left_shoulder - right_shoulder, axis=0)\n",
        "    shoulder_vec[1] = 0\n",
        "\n",
        "    norm = np.linalg.norm(shoulder_vec)\n",
        "    if norm < 1e-6:\n",
        "        shoulder_vec = np.array([1.0, 0.0, 0.0])\n",
        "    else:\n",
        "        shoulder_vec = shoulder_vec / norm\n",
        "\n",
        "    target = np.array([1.0, 0.0, 0.0])\n",
        "    v = np.cross(shoulder_vec, target)\n",
        "    c = np.dot(shoulder_vec, target)\n",
        "\n",
        "    if np.linalg.norm(v) < 1e-6:\n",
        "        R = np.eye(3)\n",
        "    else:\n",
        "        vx = np.array([[0, -v[2], v[1]],[v[2], 0, -v[0]],[-v[1], v[0], 0]])\n",
        "        R = np.eye(3) + vx + vx @ vx * (1 / (1 + c))\n",
        "    seq = seq @ R.T\n",
        "\n",
        "    # 3. Escalar por distancia entre hombros\n",
        "    shoulder_dist = np.mean(np.linalg.norm(seq[:, SHOULDER_LEFT, :] - seq[:, SHOULDER_RIGHT, :], axis=1))\n",
        "    scale = 1.0 / shoulder_dist if shoulder_dist > 1e-6 else 1.0\n",
        "    seq = seq * scale\n",
        "\n",
        "    # 4. Normalizaci√≥n de longitud de huesos (por generalizaci√≥n)\n",
        "    bones = [(SPINE_BASE, SPINE_MID), (SPINE_MID, SPINE_SHOULDER), (SPINE_SHOULDER, NECK), (NECK, HEAD),\n",
        "             (SPINE_SHOULDER, SHOULDER_LEFT), (SHOULDER_LEFT, ELBOW_LEFT), (ELBOW_LEFT, WRIST_LEFT), (WRIST_LEFT, HAND_LEFT),\n",
        "             (SPINE_SHOULDER, SHOULDER_RIGHT), (SHOULDER_RIGHT, ELBOW_RIGHT), (ELBOW_RIGHT, WRIST_RIGHT), (WRIST_RIGHT, HAND_RIGHT),\n",
        "             (SPINE_BASE, HIP_LEFT), (HIP_LEFT, KNEE_LEFT), (KNEE_LEFT, ANKLE_LEFT), (ANKLE_LEFT, FOOT_LEFT),\n",
        "             (SPINE_BASE, HIP_RIGHT), (HIP_RIGHT, KNEE_RIGHT), (KNEE_RIGHT, ANKLE_RIGHT), (ANKLE_RIGHT, FOOT_RIGHT)]\n",
        "\n",
        "    for j1, j2 in bones:\n",
        "        vec = seq[:, j2] - seq[:, j1]\n",
        "        avg_len = np.mean(np.linalg.norm(vec, axis=1))\n",
        "        if avg_len > 1e-6:\n",
        "            seq[:, j2] = seq[:, j1] + vec / avg_len\n",
        "\n",
        "    return seq"
      ],
      "metadata": {
        "id": "XNT3-u9_cSsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## üìê MAPEO DE LANDMARKS (MEDIAPIPE ‚Üí KINECT25)\n",
        "def compute_spine_points(landmarks):\n",
        "    \"\"\"Calcula puntos sint√©ticos de la columna.\"\"\"\n",
        "    def to_np(idx):\n",
        "        lm = landmarks[idx]\n",
        "        return np.array([lm.x, lm.y, lm.z], dtype=np.float32)\n",
        "\n",
        "    left_hip = to_np(mp_pose.PoseLandmark.LEFT_HIP)\n",
        "    right_hip = to_np(mp_pose.PoseLandmark.RIGHT_HIP)\n",
        "    left_sh = to_np(mp_pose.PoseLandmark.LEFT_SHOULDER)\n",
        "    right_sh = to_np(mp_pose.PoseLandmark.RIGHT_SHOULDER)\n",
        "\n",
        "    spine_base = (left_hip + right_hip) / 2.0\n",
        "    spine_shoulder = (left_sh + right_sh) / 2.0\n",
        "    spine_mid = (spine_base + spine_shoulder) / 2.0\n",
        "\n",
        "    return spine_base, spine_mid, spine_shoulder\n",
        "\n",
        "def extract_kinect25_from_mediapipe(landmarks) -> np.ndarray:\n",
        "    \"\"\"Construye un esqueleto Kinect25 (25,3) desde landmarks de MediaPipe.\"\"\"\n",
        "    def L(idx):\n",
        "        lm = landmarks[idx]\n",
        "        return np.array([lm.x, lm.y, lm.z], dtype=np.float32)\n",
        "\n",
        "    spine_base, spine_mid, spine_shoulder = compute_spine_points(landmarks)\n",
        "\n",
        "    k = np.zeros((25, 3), dtype=np.float32)\n",
        "\n",
        "    k[0] = spine_base; k[1] = spine_mid; k[2] = spine_shoulder; k[3] = L(mp_pose.PoseLandmark.NOSE)\n",
        "    k[4] = L(mp_pose.PoseLandmark.LEFT_SHOULDER); k[5] = L(mp_pose.PoseLandmark.LEFT_ELBOW); k[6] = L(mp_pose.PoseLandmark.LEFT_WRIST)\n",
        "    k[7] = L(mp_pose.PoseLandmark.LEFT_INDEX)\n",
        "    k[8] = L(mp_pose.PoseLandmark.RIGHT_SHOULDER); k[9] = L(mp_pose.PoseLandmark.RIGHT_ELBOW); k[10] = L(mp_pose.PoseLandmark.RIGHT_WRIST)\n",
        "    k[11] = L(mp_pose.PoseLandmark.RIGHT_INDEX)\n",
        "    k[12] = L(mp_pose.PoseLandmark.LEFT_HIP); k[13] = L(mp_pose.PoseLandmark.LEFT_KNEE); k[14] = L(mp_pose.PoseLandmark.LEFT_ANKLE); k[15] = L(mp_pose.PoseLandmark.LEFT_FOOT_INDEX)\n",
        "    k[16] = L(mp_pose.PoseLandmark.RIGHT_HIP); k[17] = L(mp_pose.PoseLandmark.RIGHT_KNEE); k[18] = L(mp_pose.PoseLandmark.RIGHT_ANKLE); k[19] = L(mp_pose.PoseLandmark.RIGHT_FOOT_INDEX)\n",
        "    k[20] = spine_shoulder\n",
        "    k[21] = L(mp_pose.PoseLandmark.LEFT_INDEX); k[22] = L(mp_pose.PoseLandmark.LEFT_THUMB); k[23] = L(mp_pose.PoseLandmark.RIGHT_INDEX); k[24] = L(mp_pose.PoseLandmark.RIGHT_THUMB)\n",
        "\n",
        "    return k"
      ],
      "metadata": {
        "id": "TNGRcY1UcUeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking"
      ],
      "metadata": {
        "id": "N2E5B5ofcbrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## üì¶ CHUNKING Y PREPARACI√ìN DE ENTRADAS\n",
        "def create_chunks_from_skeletons(skeletons: List[np.ndarray], chunk_size: int) -> np.ndarray:\n",
        "    \"\"\"Divide la secuencia de esqueletos en chunks y aplica padding por repetici√≥n.\"\"\"\n",
        "    if len(skeletons) == 0:\n",
        "        return np.zeros((0, chunk_size, 25, 3), dtype=np.float32)\n",
        "\n",
        "    sk_arr = np.stack(skeletons, axis=0)\n",
        "    T = sk_arr.shape[0]\n",
        "\n",
        "    chunks = []\n",
        "    for start in range(0, T, chunk_size):\n",
        "        end = start + chunk_size\n",
        "        chunk = sk_arr[start:end]\n",
        "        if chunk.shape[0] < chunk_size:\n",
        "            # Padding: repetir el √∫ltimo frame v√°lido (Estrategia de inferencia)\n",
        "            last = chunk[-1] if chunk.shape[0] > 0 else np.zeros((25,3), dtype=np.float32)\n",
        "            pad = np.repeat(last[None, :, :], chunk_size - chunk.shape[0], axis=0)\n",
        "            chunk = np.concatenate([chunk, pad], axis=0)\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return np.stack(chunks, axis=0).astype(np.float32)\n",
        "\n",
        "def prepare_chunks_for_model(chunks_4d: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Input: (N, chunk_size, 25, 3) -> Output: (N, chunk_size, 75)\"\"\"\n",
        "    N, chunk_len, J, C = chunks_4d.shape\n",
        "    return chunks_4d.reshape(N, chunk_len, J * C)"
      ],
      "metadata": {
        "id": "o-qRS6KdccsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracci√≥n y visualizaci√≥n"
      ],
      "metadata": {
        "id": "UzeESRrRcfOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## üíæ PROCESAMIENTO DE VIDEO Y EXTRACCI√ìN\n",
        "def process_video_to_kinect25_with_visuals(video_path: str, repeat_last_valid: bool = True) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Lee el video, extrae esqueletos K25 y guarda el frame BGR y los pose_landmarks para visualizaci√≥n.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"No se pudo abrir el video: {video_path}\")\n",
        "\n",
        "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1,\n",
        "                        enable_segmentation=False, min_detection_confidence=0.5,\n",
        "                        min_tracking_confidence=0.5)\n",
        "\n",
        "    frame_data = []\n",
        "    last_valid_k25 = None\n",
        "\n",
        "    print(\"‚Üí Extrayendo frames, skeletons y landmarks (BGR) para visualizaci√≥n...\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame_bgr = cap.read() # <--- FRAME BGR ORIGINAL\n",
        "        if not ret: break\n",
        "\n",
        "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB) # Para MediaPipe\n",
        "        res = pose.process(frame_rgb)\n",
        "\n",
        "        current_pose_landmarks = res.pose_landmarks if res.pose_landmarks else None\n",
        "        current_k25 = None\n",
        "\n",
        "        if res.pose_landmarks:\n",
        "            try:\n",
        "                current_k25 = extract_kinect25_from_mediapipe(res.pose_landmarks.landmark)\n",
        "                last_valid_k25 = current_k25\n",
        "            except Exception:\n",
        "                if last_valid_k25 is not None and repeat_last_valid:\n",
        "                    current_k25 = last_valid_k25.copy()\n",
        "                else:\n",
        "                    current_k25 = np.zeros((25,3), dtype=np.float32)\n",
        "        else:\n",
        "            if last_valid_k25 is not None and repeat_last_valid:\n",
        "                current_k25 = last_valid_k25.copy()\n",
        "            else:\n",
        "                current_k25 = np.zeros((25,3), dtype=np.float32)\n",
        "\n",
        "        frame_data.append({\n",
        "            'frame': frame_bgr,\n",
        "            'k25': current_k25,\n",
        "            'pose_landmarks': current_pose_landmarks # Objeto completo de LandmarkList\n",
        "        })\n",
        "\n",
        "    cap.release()\n",
        "    pose.close()\n",
        "    return frame_data"
      ],
      "metadata": {
        "id": "Qff4sb4dceSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## üé® DIBUJO Y ETIQUETADO POR CLASE\n",
        "def draw_skeleton_and_label(image: np.ndarray, pose_landmarks, label: str, color: Tuple) -> np.ndarray:\n",
        "    \"\"\"Dibuja el esqueleto de MediaPipe y la etiqueta de clasificaci√≥n, S√ìLO cambiando el color del esqueleto.\"\"\"\n",
        "\n",
        "    # Dibujar la pose de MediaPipe (sobre el frame BGR)\n",
        "    if pose_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            image,\n",
        "            pose_landmarks,\n",
        "            mp_pose.POSE_CONNECTIONS,\n",
        "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=2),\n",
        "            connection_drawing_spec=mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=2)\n",
        "        )\n",
        "\n",
        "    # A√±adir la etiqueta textual\n",
        "    text = f\"CLASE: {label}\"\n",
        "    cv2.putText(image, text, (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
        "    return image"
      ],
      "metadata": {
        "id": "1ppQza2hcjK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline de Clasificaci√≥n"
      ],
      "metadata": {
        "id": "6N16mparcmJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## üé¨ PIPELINE COMPLETO DE CLASIFICACI√ìN Y VISUALIZACI√ìN\n",
        "def classify_and_visualize_video(video_path: str, model_path: str, class_names: List[str], chunk_size: int, colors: Dict) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Ejecuta el pipeline completo (Extracci√≥n -> Chunking -> Normalizaci√≥n -> Predicci√≥n)\n",
        "    y retorna una lista de frames (np.ndarray) con el esqueleto dibujado y coloreado.\n",
        "    \"\"\"\n",
        "    # 1. Extracci√≥n y recolecci√≥n de datos\n",
        "    frame_data_list = process_video_to_kinect25_with_visuals(video_path)\n",
        "\n",
        "    skeletons = [item['k25'] for item in frame_data_list]\n",
        "    if len(skeletons) == 0:\n",
        "        raise RuntimeError(\"No se extrajeron esqueletos del video.\")\n",
        "\n",
        "    sk_arr = np.stack(skeletons, axis=0)\n",
        "    T = sk_arr.shape[0]\n",
        "\n",
        "    # 2. Chunking\n",
        "    chunks_4d = create_chunks_from_skeletons(skeletons, chunk_size=chunk_size)\n",
        "\n",
        "    # 3. Normalizaci√≥n por chunk\n",
        "    normalized_chunks = []\n",
        "    for seq in chunks_4d:\n",
        "        seq_norm = normalize_skeleton_sequence(seq)\n",
        "        normalized_chunks.append(seq_norm)\n",
        "    normalized_chunks = np.stack(normalized_chunks, axis=0)\n",
        "\n",
        "    # 4. Preparaci√≥n y Predicci√≥n\n",
        "    X = prepare_chunks_for_model(normalized_chunks)\n",
        "\n",
        "    # Aseg√∫rate de cargar el modelo en el mismo contexto de TensorFlow\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    print(\"‚Üí Clasificando...\")\n",
        "    preds = model.predict(X, verbose=0)\n",
        "    pred_inds = preds.argmax(axis=1)\n",
        "\n",
        "    # 5. Visualizaci√≥n por Frame\n",
        "    visual_frames = []\n",
        "\n",
        "    print(\"‚Üí Dibujando y coloreando frames seg√∫n predicci√≥n por chunk...\")\n",
        "\n",
        "    for i in range(preds.shape[0]):\n",
        "        chunk_start_idx = i * chunk_size\n",
        "        chunk_end_idx = min((i + 1) * chunk_size, T)\n",
        "\n",
        "        predicted_label = class_names[pred_inds[i]]\n",
        "        color = colors.get(predicted_label, (255, 255, 255)) # Color BGR\n",
        "\n",
        "        # Aplicar el color y etiqueta a todos los frames dentro del chunk\n",
        "        for j in range(chunk_start_idx, chunk_end_idx):\n",
        "            data = frame_data_list[j]\n",
        "            frame = data['frame'].copy() # Frame BGR original, NO ALTERADO\n",
        "            pose_landmarks = data['pose_landmarks']\n",
        "\n",
        "            # La funci√≥n de dibujo solo altera el esqueleto y la etiqueta\n",
        "            visual_frame = draw_skeleton_and_label(frame, pose_landmarks, predicted_label, color)\n",
        "            visual_frames.append(visual_frame)\n",
        "\n",
        "    return visual_frames"
      ],
      "metadata": {
        "id": "0FYG7AgJcos-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecuci√≥n"
      ],
      "metadata": {
        "id": "piFntPlrcrVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "## üèÅ EJECUCI√ìN DEL PIPELINE PRINCIPAL\n",
        "# ====================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not all([VIDEO_PATH, MODEL_PATH, OUTPUT_VIDEO_PATH]):\n",
        "        print(\"‚ùå Error: Por favor, configure las rutas de archivo al inicio del script.\")\n",
        "    else:\n",
        "        try:\n",
        "            # 1. Ejecutar el pipeline visual\n",
        "            visualized_frames = classify_and_visualize_video(\n",
        "                VIDEO_PATH, MODEL_PATH, CLASS_NAMES, CHUNK_SIZE, COLORS\n",
        "            )\n",
        "\n",
        "            if visualized_frames:\n",
        "                # 2. Configurar la escritura del video\n",
        "                H, W, _ = visualized_frames[0].shape\n",
        "                fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec para MP4\n",
        "\n",
        "                # Intentar obtener los FPS del video original\n",
        "                cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "                fps = cap.get(cv2.CAP_PROP_FPS) or 30 # Usar 30 FPS como fallback\n",
        "                cap.release()\n",
        "\n",
        "                out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (W, H))\n",
        "\n",
        "                # 3. Escribir los frames (ya est√°n en BGR/OpenCV compatible)\n",
        "                for frame in visualized_frames:\n",
        "                    out.write(frame)\n",
        "\n",
        "                out.release()\n",
        "                print(f\"\\n‚úÖ Video de retroalimentaci√≥n guardado en: {OUTPUT_VIDEO_PATH}\")\n",
        "            else:\n",
        "                print(\"\\n‚ùå No se generaron frames de salida.\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"\\n‚ùå Error de Ejecuci√≥n: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Ocurri√≥ un error inesperado: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CshpcRZTcqZo",
        "outputId": "2f2dffbc-6d28-44bd-9334-695a30cd6a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚Üí Extrayendo frames, skeletons y landmarks (BGR) para visualizaci√≥n...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚Üí Clasificando...\n",
            "‚Üí Dibujando y coloreando frames seg√∫n predicci√≥n por chunk...\n",
            "\n",
            "‚úÖ Video de retroalimentaci√≥n guardado en: /content/drive/MyDrive/Videos SkillTalk/president-trump-s-midterm-rally-tour_qwl2YURk_retro.mp4\n"
          ]
        }
      ]
    }
  ]
}